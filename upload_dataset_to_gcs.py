#!/usr/bin/env python3
"""
Upload dataset to Google Cloud Storage
Uses service account authentication for secure upload
"""

import os
import json
import subprocess
import logging
import argparse
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_command(command, check=True):
    """Run a shell command and return the result"""
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        if check and result.returncode != 0:
            logger.error(f"Command failed: {command}")
            logger.error(f"Error: {result.stderr}")
            raise subprocess.CalledProcessError(result.returncode, command)
        return result
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed: {e}")
        raise

def check_gcloud_installation():
    """Check if gcloud CLI is installed"""
    try:
        result = run_command("gcloud --version", check=False)
        if result.returncode == 0:
            logger.info("‚úÖ gcloud CLI is installed")
            return True
        else:
            logger.error("‚ùå gcloud CLI is not installed")
            return False
    except FileNotFoundError:
        logger.error("‚ùå gcloud CLI is not found in PATH")
        return False

def authenticate_gcloud(key_file_path):
    """Authenticate gcloud with service account key"""
    try:
        run_command(f"gcloud auth activate-service-account --key-file={key_file_path}")
        logger.info("‚úÖ Authenticated with service account")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå Authentication failed: {e}")
        return False

def set_project(project_id):
    """Set the GCP project"""
    try:
        run_command(f"gcloud config set project {project_id}")
        logger.info(f"‚úÖ Set project to: {project_id}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå Failed to set project: {e}")
        return False

def create_bucket(bucket_name, region="us-central1"):
    """Create a GCS bucket if it doesn't exist"""
    try:
        # Check if bucket exists
        result = run_command(f"gsutil ls -b gs://{bucket_name}", check=False)
        if result.returncode == 0:
            logger.info(f"‚úÖ Bucket already exists: gs://{bucket_name}")
            return True
        
        # Create bucket
        run_command(f"gsutil mb -p {run_command('gcloud config get-value project').stdout.strip()} -c STANDARD -l {region} gs://{bucket_name}")
        logger.info(f"‚úÖ Created bucket: gs://{bucket_name}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå Failed to create bucket: {e}")
        return False

def upload_dataset(local_path, bucket_name, remote_path=None):
    """Upload dataset to GCS using rsync"""
    try:
        if remote_path is None:
            remote_path = os.path.basename(local_path)
        
        gs_path = f"gs://{bucket_name}/{remote_path}"
        
        logger.info(f"üì§ Uploading {local_path} to {gs_path}")
        
        # Use rsync for efficient upload
        run_command(f"gsutil -m rsync -r {local_path} {gs_path}")
        
        logger.info(f"‚úÖ Upload completed: {gs_path}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå Upload failed: {e}")
        return False

def verify_upload(bucket_name, remote_path):
    """Verify the upload by listing files"""
    try:
        gs_path = f"gs://{bucket_name}/{remote_path}"
        result = run_command(f"gsutil ls -r {gs_path}")
        
        files = result.stdout.strip().split('\n')
        file_count = len([f for f in files if f and not f.endswith('/')])
        
        logger.info(f"‚úÖ Upload verified: {file_count} files in {gs_path}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå Verification failed: {e}")
        return False

def generate_download_script(bucket_name, remote_path, local_path):
    """Generate a download script for use on cloud VMs"""
    download_script = f"""#!/usr/bin/env python3
\"\"\"
Download dataset from Google Cloud Storage
Generated by upload_dataset_to_gcs.py
\"\"\"

import os
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_dataset():
    \"\"\"Download dataset from GCS\"\"\"
    bucket_name = "{bucket_name}"
    remote_path = "{remote_path}"
    local_path = "{local_path}"
    
    # Create local directory
    os.makedirs(local_path, exist_ok=True)
    
    # Download using gsutil
    gs_path = f"gs://{{bucket_name}}/{{remote_path}}"
    cmd = f"gsutil -m rsync -r {{gs_path}} {{local_path}}"
    
    logger.info(f"Downloading {{gs_path}} to {{local_path}}")
    subprocess.run(cmd, shell=True, check=True)
    logger.info("Download completed!")

if __name__ == "__main__":
    download_dataset()
"""
    
    script_path = "download_dataset_from_gcs.py"
    try:
        with open(script_path, 'w') as f:
            f.write(download_script)
        
        # Make executable
        os.chmod(script_path, 0o755)
        
        logger.info(f"‚úÖ Generated download script: {script_path}")
        return script_path
    except Exception as e:
        logger.error(f"‚ùå Failed to generate download script: {e}")
        return None

def load_config(config_file="gcp_config.json"):
    """Load configuration from file"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        logger.info(f"‚úÖ Loaded config from: {config_file}")
        return config
    except FileNotFoundError:
        logger.error(f"‚ùå Config file not found: {config_file}")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Invalid config file: {e}")
        return None

def main():
    """Main upload function"""
    parser = argparse.ArgumentParser(description='Upload dataset to Google Cloud Storage')
    parser.add_argument('--dataset-path', required=True, help='Path to local dataset directory')
    parser.add_argument('--key-file', default='dataset-uploader-key.json', help='Service account key file')
    parser.add_argument('--config', default='gcp_config.json', help='Configuration file')
    parser.add_argument('--remote-path', help='Remote path in bucket (default: dataset name)')
    parser.add_argument('--bucket-name', help='GCS bucket name (overrides config)')
    
    args = parser.parse_args()
    
    logger.info("üöÄ Starting dataset upload to Google Cloud Storage")
    
    # Check prerequisites
    if not check_gcloud_installation():
        return False
    
    # Load configuration
    config = load_config(args.config)
    if not config:
        logger.error("Please run setup_gcp_auth.py first to create configuration")
        return False
    
    # Use command line args or config
    key_file = args.key_file if os.path.exists(args.key_file) else config.get('key_file', args.key_file)
    bucket_name = args.bucket_name or config.get('bucket_name', 'face-training-datasets')
    project_id = config.get('project_id')
    
    if not project_id:
        logger.error("No project ID found in config")
        return False
    
    try:
        # Authenticate
        if not authenticate_gcloud(key_file):
            return False
        
        # Set project
        if not set_project(project_id):
            return False
        
        # Create bucket if needed
        if not create_bucket(bucket_name):
            return False
        
        # Upload dataset
        local_path = args.dataset_path
        remote_path = args.remote_path or os.path.basename(local_path)
        
        if not upload_dataset(local_path, bucket_name, remote_path):
            return False
        
        # Verify upload
        if not verify_upload(bucket_name, remote_path):
            return False
        
        # Generate download script
        download_script = generate_download_script(bucket_name, remote_path, local_path)
        
        logger.info("üéâ Dataset upload completed successfully!")
        logger.info(f"üìÅ Dataset location: gs://{bucket_name}/{remote_path}")
        logger.info(f"üì• Download script: {download_script}")
        logger.info("üí° Use the download script on your cloud VM to download the dataset")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Upload failed: {e}")
        return False

if __name__ == "__main__":
    main() 